{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf7ba73",
   "metadata": {},
   "source": [
    "# 1.0 — Collecte de données marché (Yahoo Finance)\n",
    "\n",
    "\n",
    "**Explication :** \n",
    "- On les télécharge les données du S&p 500, SPY, VIX, depuis Yehoo Finance ( on utilise la librairie), \n",
    "- On normalise les colonnes pour éviter tout problèmes et par la suite on sauvegarde les CSV et les Parquets en Json pour les traiter par la suite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be33676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "TICKERS = {\"SPX_INDEX\": \"^GSPC\", \"SPY_ETF\": \"SPY\", \"VIX_INDEX\": \"^VIX\"}\n",
    "START_DATE = \"1993-01-29\"\n",
    "END_DATE = None  # None = jusqu'à aujourd'hui\n",
    "DATA_DIR = Path(\"data/raw\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f08e0",
   "metadata": {},
   "source": [
    "## 1.1 Fonction \"fetch\" — téléchargement & normalisation\n",
    "\n",
    "**Explication :**\n",
    "1) On télécharge les données, et on utilise auto_adjust=True pour la colonnes du prix de clotures pour avoir un pric corrigé des splits.  \n",
    "2) Aplatis les colonnes et enleve les suffixes ticker (code produit d'un actif financier ).\n",
    "3) Force la présence de la variable \"date\" et adj_close.\n",
    "4) On trie les lignes par date chronologie.\n",
    "5) On supprime les données \"NaN\"\n",
    "\n",
    "**Pourquoi nous avons fait cela :** \n",
    "1) On utilise auto_adjust=True pour ensuite faciliter le calcul des rendements mais aussi éviter de gérer manuellement la colonne \"close\", cela permet de garantir que le prix utilisé reflete bien la réalité économique (données à jours).\n",
    "2) \"Aplatir les colonnes + supprimer les suffixes\" -> Permet de rendre le dataset homogène et évite d'avoir des noms de colonnes différents selon l'actif.\n",
    "3) \"Forcer la présence de date et adj_close\" -> Assure d'avoir une structure standardisé pour les futures traitements.\n",
    "4) \"Trier par date\" -> garantit une série temporelle strictement chronologique.\n",
    "5) \"Supprimer les NaN\" -> Empeche la propagation d'erreurs statistiques \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(ticker: str, start: str, end: str | None) -> pd.DataFrame:\n",
    "\n",
    "    df = yf.download(\n",
    "        ticker, start=start, end=end,\n",
    "        auto_adjust=True,        \n",
    "        group_by=\"column\",\n",
    "        actions=False,\n",
    "        progress=False,\n",
    "        threads=True,            \n",
    "    )\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(f\"Aucune donnée pour {ticker}\")\n",
    "\n",
    "\n",
    "    \n",
    "    df = df.dropna(how=\"all\").reset_index()\n",
    "    def _flatten_cols(cols):\n",
    "        out = []\n",
    "        for c in cols:\n",
    "            if isinstance(c, tuple):\n",
    "                s = \"_\".join([str(x) for x in c if x not in (None, \"\", \"None\")])\n",
    "            else:\n",
    "                s = str(c)\n",
    "            out.append(s.lower().replace(\" \", \"_\"))\n",
    "        return out\n",
    "    df.columns = _flatten_cols(df.columns)\n",
    "\n",
    "\n",
    "    suf = \"_\" + ticker.lower()\n",
    "    rename_map = {}\n",
    "    for c in df.columns:\n",
    "        if c in (\"date\", \"index\"):\n",
    "            continue\n",
    "        base = c[:-len(suf)] if c.endswith(suf) else c\n",
    "        rename_map[c] = base\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    if \"date\" not in df.columns and \"index\" in df.columns:\n",
    "        df = df.rename(columns={\"index\": \"date\"})\n",
    "    if \"adj_close\" not in df.columns and \"close\" in df.columns:\n",
    "        df[\"adj_close\"] = df[\"close\"]\n",
    "    if \"adj_close\" not in df.columns and \"close\" not in df.columns:\n",
    "        raise KeyError(f\"Pas de 'close'/'adj_close' après normalisation. Colonnes: {list(df.columns)}\")\n",
    "\n",
    "    df = df.sort_values(\"date\").dropna(subset=[\"adj_close\"]).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15217a50",
   "metadata": {},
   "source": [
    "## 1.2 Fonction save — sauvegarde & métadonnées\n",
    "\n",
    "**Explication :**\n",
    "1) Sauvegarde le DataFrame en CSV (lisible partout) et en Parquet (format optimisé).\n",
    "2) Retourne un dictionnaire de métadonnées :\n",
    "    - nombre de lignes,\n",
    "    - première et dernière date de la série\n",
    "    - chemins des fichiers écrits\n",
    "    - liste des colonnes.\n",
    "\n",
    "**Pourquoi nous avons fait cela :** \n",
    "1) Un a un CSV comme format universel et qu'il soit lisible facilement, et un format \"Parquet\" qui est un format optimisé (donc rapide)\n",
    "2) Permet de retourner des métadonnées et permet de vérifier rapidement la cohérence des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e6d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save(df: pd.DataFrame, name: str) -> dict:\n",
    "    p_csv = DATA_DIR / f\"{name}.csv\"\n",
    "    p_par = DATA_DIR / f\"{name}.parquet\"\n",
    "    df.to_csv(p_csv, index=False)\n",
    "    df.to_parquet(p_par, index=False)\n",
    "    return {\n",
    "        \"rows\": len(df),\n",
    "        \"first_date\": df[\"date\"].iloc[0].strftime(\"%Y-%m-%d\"),\n",
    "        \"last_date\": df[\"date\"].iloc[-1].strftime(\"%Y-%m-%d\"),\n",
    "        \"csv\": str(p_csv),\n",
    "        \"parquet\": str(p_par),\n",
    "        \"columns\": df.columns.tolist(),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06aa507",
   "metadata": {},
   "source": [
    "## 1.3 Bloc principal — exécution & génération des métadonnées\n",
    "\n",
    "**Explication :**\n",
    "1) On créer un dictionnaire qui stocke tout les données.\n",
    "2) Pour chaque ticker on télécharge les donnés via à la fonction \"fetch\" et on sauvegarde via la fonction \"save\".\n",
    "3) On écrit un fichier \"metadata.json\" et un récapitulatif dans \"DATA_DIR\"\n",
    "4) Créer un message message de confirmation en fin de collecte.\n",
    "\n",
    "\n",
    "**Pourquoi nous avons fait cela :** \n",
    "1) Générer le fichier \"meta\" centralise toutes les informations de traçabilité (source, période, tickers, options).\n",
    "2) Permet de rendre la collecte scalable (ajouter un actif = 1 ligne dans la config).\n",
    "3) Permet de faciliter la reproductibilité et le debug.\n",
    "4) Le print final sert de feedback rapide pour confirmer que la collecte s’est bien déroulée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c3da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Téléchargement SPX_INDEX (^GSPC)\n",
      "→ Téléchargement SPY_ETF (SPY)\n",
      "→ Téléchargement VIX_INDEX (^VIX)\n",
      "✅ Collecte terminée → C:\\Users\\antoi\\Proton Drive\\antoinebuisson000\\My files\\12 - SKEMA\\4 - CARRIERE FINANCE\\1 - PROJET FINANCE QUANT\\notebook\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    meta = {\n",
    "        \"collected_at_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"source\": \"Yahoo Finance via yfinance\",\n",
    "        \"universe\": [\"^GSPC\", \"SPY\", \"^VIX\"],\n",
    "        \"period\": {\"start\": START_DATE, \"end\": END_DATE or \"today\"},\n",
    "        \"datasets\": {},\n",
    "        \"notes\": [\n",
    "            \"Colonnes aplanies pour éviter MultiIndex.\",\n",
    "            \"Prix ajustés (auto_adjust=True) pour des rendements cohérents.\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for name, ticker in TICKERS.items():\n",
    "        print(f\"→ Téléchargement {name} ({ticker})\")\n",
    "        df = fetch(ticker, START_DATE, END_DATE)\n",
    "        meta[\"datasets\"][name] = {\"ticker\": ticker, **save(df, name.lower())}\n",
    "\n",
    "    (DATA_DIR / \"metadata.json\").write_text(\n",
    "        json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"✅ Collecte terminée →\", DATA_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd14cb",
   "metadata": {},
   "source": [
    "### 1.4 Chargement des données brutes — inspection & métadonnées\n",
    "\n",
    "**Explication :**\n",
    "1) Affiche le contenu du dossier data/raw pour vérifier quels fichiers ont été collectés.\n",
    "2) Lit le fichier metadata.json et le charge en dictionnaire Python m.\n",
    "3) Affiche la structure des métadonnées (aperçu du contenu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fed9887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du dossier: [WindowsPath('data/raw/metadata.json'), WindowsPath('data/raw/spx_index.csv'), WindowsPath('data/raw/spx_index.parquet'), WindowsPath('data/raw/spy_etf.csv'), WindowsPath('data/raw/spy_etf.parquet'), WindowsPath('data/raw/vix_index.csv'), WindowsPath('data/raw/vix_index.parquet')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'collected_at_utc': '2025-09-18T10:30:48+00:00',\n",
       " 'source': 'Yahoo Finance via yfinance',\n",
       " 'universe': ['^GSPC', 'SPY', '^VIX'],\n",
       " 'period': {'start': '1993-01-29', 'end': 'today'},\n",
       " 'datasets': {'SPX_INDEX': {'ticker': '^GSPC',\n",
       "   'rows': 8215,\n",
       "   'first_date': '1993-01-29',\n",
       "   'last_date': '2025-09-17',\n",
       "   'csv': 'data\\\\raw\\\\spx_index.csv',\n",
       "   'parquet': 'data\\\\raw\\\\spx_index.parquet',\n",
       "   'columns': ['date', 'close', 'high', 'low', 'open', 'volume', 'adj_close']},\n",
       "  'SPY_ETF': {'ticker': 'SPY',\n",
       "   'rows': 8215,\n",
       "   'first_date': '1993-01-29',\n",
       "   'last_date': '2025-09-17',\n",
       "   'csv': 'data\\\\raw\\\\spy_etf.csv',\n",
       "   'parquet': 'data\\\\raw\\\\spy_etf.parquet',\n",
       "   'columns': ['date', 'close', 'high', 'low', 'open', 'volume', 'adj_close']},\n",
       "  'VIX_INDEX': {'ticker': '^VIX',\n",
       "   'rows': 8216,\n",
       "   'first_date': '1993-01-29',\n",
       "   'last_date': '2025-09-18',\n",
       "   'csv': 'data\\\\raw\\\\vix_index.csv',\n",
       "   'parquet': 'data\\\\raw\\\\vix_index.parquet',\n",
       "   'columns': ['date',\n",
       "    'close',\n",
       "    'high',\n",
       "    'low',\n",
       "    'open',\n",
       "    'volume',\n",
       "    'adj_close']}},\n",
       " 'notes': ['Colonnes aplanies pour éviter MultiIndex.',\n",
       "  'Prix ajustés (auto_adjust=True) pour des rendements cohérents.']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Contenu du dossier:\", list(Path(\"data/raw\").glob(\"*\")))\n",
    "m = json.loads(Path(\"data/raw/metadata.json\").read_text(encoding=\"utf-8\"))\n",
    "m\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
